{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')  \n",
    "from _metrics import  CrossValidation, accuracy_score\n",
    "from  _logistic import LogisticRegression\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import _dgp as dg\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Data loading and preparation**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I generate data with 50 variables, among which there are 5 true first variables and target variable with 2 classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "x,y=dg.gen(type=1,regression=\"logistic\")\n",
    "columns=[f\"column_{i}\"for i in range(1,51)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Estimation**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now apply simple Gradient descent optimisation algorithm to estimate B parameter of Logistic Regression "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "algorithm did not converge under 100 iterations,so the calculated parameter is biased\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[-0.26201153],\n",
       "       [ 1.85141827],\n",
       "       [ 0.8778802 ],\n",
       "       [ 0.3764929 ],\n",
       "       [-0.27101373]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model1=LogisticRegression(solver=\"gd\",add_intercept=True,)\n",
    "model1.fit(x,y)[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Algorithm could not converge:**\n",
    "If you have this message, **NEVER** proceed to further steps as \n",
    "the estimated parameter is biased. \n",
    "\n",
    "Therefore, try other hyperparameters or optimisations to aim  convergence.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example stochastic gradient descent with another learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "algorithm did not converge under 1600 iterations,so the calculated parameter is biased\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[-0.35374579],\n",
       "       [ 2.21888993],\n",
       "       [ 1.08833334],\n",
       "       [ 0.44701799],\n",
       "       [-0.32078771]])"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2=LogisticRegression(solver=\"sgd\",learning_rate=0.002,add_intercept=True,)\n",
    "model2.fit(x,y)[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or Newton Raphson, but before doing it, please verify that the inverse of x.T@x exists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.7458538617650583e+133"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.linalg.det(x.T@x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "algorithm did  converge under 100 iterations (at 7 iterations)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[-0.50411425],\n",
       "       [ 2.81552631],\n",
       "       [ 1.419166  ],\n",
       "       [ 0.55959778],\n",
       "       [-0.40066804]])"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model3=LogisticRegression(solver=\"nr\",learning_rate=0.001,add_intercept=True,)\n",
    "model3.fit(x,y)[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we see, NR optimisation is much faster than simple GD, so we will continue using `model3` in the nest steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Prediction**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The prediction is done by applying softmax activation function to the linear regression x@B:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2.06658740e-03, 9.97933413e-01],\n",
       "       [2.80964489e-01, 7.19035511e-01],\n",
       "       [9.99366221e-01, 6.33778571e-04],\n",
       "       [8.66956196e-01, 1.33043804e-01],\n",
       "       [4.42909596e-02, 9.55709040e-01]])"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions=model3.predict(x)\n",
    "model3.proba[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the first column corresponding to the probability of class 1 to be observed, the second column- of class 0.\n",
    "\n",
    "Then we will compare both probabilities and assign the class based on the higher one"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**When we predict the model, we have to evaluate the model \"quality\". This can be done using several approaches**:\n",
    " - Evaluating metrics such as Accuracy, F1 score etc ...\n",
    " - Evaluating information criteria such as AIC, BIC, adjusted R2 ...\n",
    " - Evaluating significance of B estimator which is a traditional econometrics approach."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this secton we will discover first method, other two methods will be presented in the next section **\"Model Interpretability\"**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, here we want to see if our predictions are correct, that is why we will compare real values with predicted values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 1., 1., 0., 1., 0., 0., 1., 0.])"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#predicted values\n",
    "predictions[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 1, 0, 0, 0, 0, 1, 1, 0])"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#real values\n",
    "y[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Before doing the comparison, we have to see if classes {0,1} are balanced.\n",
    "\n",
    " If yes then the use of `accuracy_score` will be enough, otherwise we will use other metrics such as \n",
    " `f1_score`or `recall_score`or `precision_score`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.912"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(model3.predict(x),y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This one-shot evaluation is good, but not robust against heterogeneity in the data, so we will couple it with `CrossValidation` technique and then average CV scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "algorithm did  converge under 100 iterations (at 8 iterations)\n",
      "algorithm did  converge under 100 iterations (at 8 iterations)\n",
      "algorithm did  converge under 100 iterations (at 8 iterations)\n",
      "algorithm did  converge under 100 iterations (at 8 iterations)\n",
      "[0.792, 0.808, 0.856, 0.824]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.82"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#nb of folds is 4 so we will run 4 models \n",
    "CV_scores=CrossValidation(Class_algorithm=model3,x=x,y=y,metrics_function=accuracy_score,nb_k_fold=4)\n",
    "print(CV_scores)\n",
    "np.mean(CV_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Model Interpretability**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The use of Logistic Regression is very popular because it allows to interpret coefficients as in Linear Regression. So we can do it too:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>coef</th>\n",
       "      <th>std</th>\n",
       "      <th>z value</th>\n",
       "      <th>p value</th>\n",
       "      <th>p value star</th>\n",
       "      <th>odds ratio</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>intercept_1</th>\n",
       "      <td>-0.5041</td>\n",
       "      <td>0.2</td>\n",
       "      <td>-2.56</td>\n",
       "      <td>0.011</td>\n",
       "      <td>**</td>\n",
       "      <td>0.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>column_1_1</th>\n",
       "      <td>2.8155</td>\n",
       "      <td>0.32</td>\n",
       "      <td>8.72</td>\n",
       "      <td>0.0</td>\n",
       "      <td>***</td>\n",
       "      <td>16.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>column_2_1</th>\n",
       "      <td>1.4192</td>\n",
       "      <td>0.24</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>***</td>\n",
       "      <td>4.13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>column_3_1</th>\n",
       "      <td>0.5596</td>\n",
       "      <td>0.21</td>\n",
       "      <td>2.61</td>\n",
       "      <td>0.009</td>\n",
       "      <td>***</td>\n",
       "      <td>1.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>column_4_1</th>\n",
       "      <td>-0.4007</td>\n",
       "      <td>0.19</td>\n",
       "      <td>-2.1</td>\n",
       "      <td>0.037</td>\n",
       "      <td>**</td>\n",
       "      <td>0.67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>column_5_1</th>\n",
       "      <td>-4.0397</td>\n",
       "      <td>0.45</td>\n",
       "      <td>-8.92</td>\n",
       "      <td>0.0</td>\n",
       "      <td>***</td>\n",
       "      <td>0.02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>column_6_1</th>\n",
       "      <td>0.3089</td>\n",
       "      <td>0.2</td>\n",
       "      <td>1.52</td>\n",
       "      <td>0.129</td>\n",
       "      <td></td>\n",
       "      <td>1.36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>column_7_1</th>\n",
       "      <td>-0.7026</td>\n",
       "      <td>0.23</td>\n",
       "      <td>-3.11</td>\n",
       "      <td>0.002</td>\n",
       "      <td>***</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>column_8_1</th>\n",
       "      <td>-0.1147</td>\n",
       "      <td>0.2</td>\n",
       "      <td>-0.58</td>\n",
       "      <td>0.565</td>\n",
       "      <td></td>\n",
       "      <td>0.89</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>column_9_1</th>\n",
       "      <td>0.0897</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.48</td>\n",
       "      <td>0.629</td>\n",
       "      <td></td>\n",
       "      <td>1.09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>column_10_1</th>\n",
       "      <td>-0.2684</td>\n",
       "      <td>0.2</td>\n",
       "      <td>-1.33</td>\n",
       "      <td>0.185</td>\n",
       "      <td></td>\n",
       "      <td>0.76</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>column_11_1</th>\n",
       "      <td>-0.3067</td>\n",
       "      <td>0.2</td>\n",
       "      <td>-1.52</td>\n",
       "      <td>0.13</td>\n",
       "      <td></td>\n",
       "      <td>0.74</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>column_12_1</th>\n",
       "      <td>0.2999</td>\n",
       "      <td>0.18</td>\n",
       "      <td>1.66</td>\n",
       "      <td>0.098</td>\n",
       "      <td>*</td>\n",
       "      <td>1.35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>column_13_1</th>\n",
       "      <td>-0.4047</td>\n",
       "      <td>0.19</td>\n",
       "      <td>-2.09</td>\n",
       "      <td>0.037</td>\n",
       "      <td>**</td>\n",
       "      <td>0.67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>column_14_1</th>\n",
       "      <td>-0.0524</td>\n",
       "      <td>0.2</td>\n",
       "      <td>-0.26</td>\n",
       "      <td>0.792</td>\n",
       "      <td></td>\n",
       "      <td>0.95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>column_15_1</th>\n",
       "      <td>-0.433</td>\n",
       "      <td>0.19</td>\n",
       "      <td>-2.3</td>\n",
       "      <td>0.022</td>\n",
       "      <td>**</td>\n",
       "      <td>0.65</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>column_16_1</th>\n",
       "      <td>-0.229</td>\n",
       "      <td>0.19</td>\n",
       "      <td>-1.18</td>\n",
       "      <td>0.238</td>\n",
       "      <td></td>\n",
       "      <td>0.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>column_17_1</th>\n",
       "      <td>0.011</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.953</td>\n",
       "      <td></td>\n",
       "      <td>1.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>column_18_1</th>\n",
       "      <td>0.1072</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.57</td>\n",
       "      <td>0.57</td>\n",
       "      <td></td>\n",
       "      <td>1.11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>column_19_1</th>\n",
       "      <td>-0.0747</td>\n",
       "      <td>0.17</td>\n",
       "      <td>-0.44</td>\n",
       "      <td>0.659</td>\n",
       "      <td></td>\n",
       "      <td>0.93</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>column_20_1</th>\n",
       "      <td>-0.0409</td>\n",
       "      <td>0.2</td>\n",
       "      <td>-0.21</td>\n",
       "      <td>0.834</td>\n",
       "      <td></td>\n",
       "      <td>0.96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>column_21_1</th>\n",
       "      <td>0.1521</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.79</td>\n",
       "      <td>0.431</td>\n",
       "      <td></td>\n",
       "      <td>1.16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>column_22_1</th>\n",
       "      <td>-0.2729</td>\n",
       "      <td>0.19</td>\n",
       "      <td>-1.46</td>\n",
       "      <td>0.146</td>\n",
       "      <td></td>\n",
       "      <td>0.76</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>column_23_1</th>\n",
       "      <td>-0.0843</td>\n",
       "      <td>0.21</td>\n",
       "      <td>-0.4</td>\n",
       "      <td>0.686</td>\n",
       "      <td></td>\n",
       "      <td>0.92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>column_24_1</th>\n",
       "      <td>-0.3183</td>\n",
       "      <td>0.22</td>\n",
       "      <td>-1.46</td>\n",
       "      <td>0.144</td>\n",
       "      <td></td>\n",
       "      <td>0.73</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>column_25_1</th>\n",
       "      <td>-0.2431</td>\n",
       "      <td>0.2</td>\n",
       "      <td>-1.21</td>\n",
       "      <td>0.226</td>\n",
       "      <td></td>\n",
       "      <td>0.78</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>column_26_1</th>\n",
       "      <td>0.3631</td>\n",
       "      <td>0.22</td>\n",
       "      <td>1.69</td>\n",
       "      <td>0.092</td>\n",
       "      <td>*</td>\n",
       "      <td>1.44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>column_27_1</th>\n",
       "      <td>-0.1852</td>\n",
       "      <td>0.21</td>\n",
       "      <td>-0.9</td>\n",
       "      <td>0.367</td>\n",
       "      <td></td>\n",
       "      <td>0.83</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>column_28_1</th>\n",
       "      <td>0.2104</td>\n",
       "      <td>0.2</td>\n",
       "      <td>1.04</td>\n",
       "      <td>0.298</td>\n",
       "      <td></td>\n",
       "      <td>1.23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>column_29_1</th>\n",
       "      <td>-0.2582</td>\n",
       "      <td>0.2</td>\n",
       "      <td>-1.32</td>\n",
       "      <td>0.187</td>\n",
       "      <td></td>\n",
       "      <td>0.77</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>column_30_1</th>\n",
       "      <td>0.2441</td>\n",
       "      <td>0.2</td>\n",
       "      <td>1.22</td>\n",
       "      <td>0.222</td>\n",
       "      <td></td>\n",
       "      <td>1.28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>column_31_1</th>\n",
       "      <td>-0.0049</td>\n",
       "      <td>0.18</td>\n",
       "      <td>-0.03</td>\n",
       "      <td>0.979</td>\n",
       "      <td></td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>column_32_1</th>\n",
       "      <td>0.1585</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.423</td>\n",
       "      <td></td>\n",
       "      <td>1.17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>column_33_1</th>\n",
       "      <td>0.1147</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.61</td>\n",
       "      <td>0.544</td>\n",
       "      <td></td>\n",
       "      <td>1.12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>column_34_1</th>\n",
       "      <td>-0.0283</td>\n",
       "      <td>0.19</td>\n",
       "      <td>-0.15</td>\n",
       "      <td>0.883</td>\n",
       "      <td></td>\n",
       "      <td>0.97</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>column_35_1</th>\n",
       "      <td>-0.0086</td>\n",
       "      <td>0.2</td>\n",
       "      <td>-0.04</td>\n",
       "      <td>0.965</td>\n",
       "      <td></td>\n",
       "      <td>0.99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>column_36_1</th>\n",
       "      <td>-0.0773</td>\n",
       "      <td>0.19</td>\n",
       "      <td>-0.4</td>\n",
       "      <td>0.69</td>\n",
       "      <td></td>\n",
       "      <td>0.93</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>column_37_1</th>\n",
       "      <td>0.0843</td>\n",
       "      <td>0.18</td>\n",
       "      <td>0.47</td>\n",
       "      <td>0.64</td>\n",
       "      <td></td>\n",
       "      <td>1.09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>column_38_1</th>\n",
       "      <td>-0.2124</td>\n",
       "      <td>0.2</td>\n",
       "      <td>-1.08</td>\n",
       "      <td>0.281</td>\n",
       "      <td></td>\n",
       "      <td>0.81</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>column_39_1</th>\n",
       "      <td>0.2837</td>\n",
       "      <td>0.19</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.135</td>\n",
       "      <td></td>\n",
       "      <td>1.33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>column_40_1</th>\n",
       "      <td>0.2625</td>\n",
       "      <td>0.17</td>\n",
       "      <td>1.52</td>\n",
       "      <td>0.13</td>\n",
       "      <td></td>\n",
       "      <td>1.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>column_41_1</th>\n",
       "      <td>0.0162</td>\n",
       "      <td>0.18</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.93</td>\n",
       "      <td></td>\n",
       "      <td>1.02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>column_42_1</th>\n",
       "      <td>0.1502</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.77</td>\n",
       "      <td>0.439</td>\n",
       "      <td></td>\n",
       "      <td>1.16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>column_43_1</th>\n",
       "      <td>0.3367</td>\n",
       "      <td>0.21</td>\n",
       "      <td>1.6</td>\n",
       "      <td>0.111</td>\n",
       "      <td></td>\n",
       "      <td>1.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>column_44_1</th>\n",
       "      <td>0.3104</td>\n",
       "      <td>0.18</td>\n",
       "      <td>1.69</td>\n",
       "      <td>0.093</td>\n",
       "      <td>*</td>\n",
       "      <td>1.36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>column_45_1</th>\n",
       "      <td>0.0132</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.944</td>\n",
       "      <td></td>\n",
       "      <td>1.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>column_46_1</th>\n",
       "      <td>-0.3019</td>\n",
       "      <td>0.2</td>\n",
       "      <td>-1.49</td>\n",
       "      <td>0.136</td>\n",
       "      <td></td>\n",
       "      <td>0.74</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>column_47_1</th>\n",
       "      <td>0.0215</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.11</td>\n",
       "      <td>0.912</td>\n",
       "      <td></td>\n",
       "      <td>1.02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>column_48_1</th>\n",
       "      <td>-0.0671</td>\n",
       "      <td>0.22</td>\n",
       "      <td>-0.31</td>\n",
       "      <td>0.758</td>\n",
       "      <td></td>\n",
       "      <td>0.94</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>column_49_1</th>\n",
       "      <td>0.0232</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.903</td>\n",
       "      <td></td>\n",
       "      <td>1.02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>column_50_1</th>\n",
       "      <td>-0.1686</td>\n",
       "      <td>0.19</td>\n",
       "      <td>-0.88</td>\n",
       "      <td>0.38</td>\n",
       "      <td></td>\n",
       "      <td>0.84</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                coef   std z value p value p value star odds ratio\n",
       "intercept_1  -0.5041   0.2   -2.56   0.011           **        0.6\n",
       "column_1_1    2.8155  0.32    8.72     0.0          ***       16.7\n",
       "column_2_1    1.4192  0.24     6.0     0.0          ***       4.13\n",
       "column_3_1    0.5596  0.21    2.61   0.009          ***       1.75\n",
       "column_4_1   -0.4007  0.19    -2.1   0.037           **       0.67\n",
       "column_5_1   -4.0397  0.45   -8.92     0.0          ***       0.02\n",
       "column_6_1    0.3089   0.2    1.52   0.129                    1.36\n",
       "column_7_1   -0.7026  0.23   -3.11   0.002          ***        0.5\n",
       "column_8_1   -0.1147   0.2   -0.58   0.565                    0.89\n",
       "column_9_1    0.0897  0.19    0.48   0.629                    1.09\n",
       "column_10_1  -0.2684   0.2   -1.33   0.185                    0.76\n",
       "column_11_1  -0.3067   0.2   -1.52    0.13                    0.74\n",
       "column_12_1   0.2999  0.18    1.66   0.098            *       1.35\n",
       "column_13_1  -0.4047  0.19   -2.09   0.037           **       0.67\n",
       "column_14_1  -0.0524   0.2   -0.26   0.792                    0.95\n",
       "column_15_1   -0.433  0.19    -2.3   0.022           **       0.65\n",
       "column_16_1   -0.229  0.19   -1.18   0.238                     0.8\n",
       "column_17_1    0.011  0.19    0.06   0.953                    1.01\n",
       "column_18_1   0.1072  0.19    0.57    0.57                    1.11\n",
       "column_19_1  -0.0747  0.17   -0.44   0.659                    0.93\n",
       "column_20_1  -0.0409   0.2   -0.21   0.834                    0.96\n",
       "column_21_1   0.1521  0.19    0.79   0.431                    1.16\n",
       "column_22_1  -0.2729  0.19   -1.46   0.146                    0.76\n",
       "column_23_1  -0.0843  0.21    -0.4   0.686                    0.92\n",
       "column_24_1  -0.3183  0.22   -1.46   0.144                    0.73\n",
       "column_25_1  -0.2431   0.2   -1.21   0.226                    0.78\n",
       "column_26_1   0.3631  0.22    1.69   0.092            *       1.44\n",
       "column_27_1  -0.1852  0.21    -0.9   0.367                    0.83\n",
       "column_28_1   0.2104   0.2    1.04   0.298                    1.23\n",
       "column_29_1  -0.2582   0.2   -1.32   0.187                    0.77\n",
       "column_30_1   0.2441   0.2    1.22   0.222                    1.28\n",
       "column_31_1  -0.0049  0.18   -0.03   0.979                     1.0\n",
       "column_32_1   0.1585   0.2     0.8   0.423                    1.17\n",
       "column_33_1   0.1147  0.19    0.61   0.544                    1.12\n",
       "column_34_1  -0.0283  0.19   -0.15   0.883                    0.97\n",
       "column_35_1  -0.0086   0.2   -0.04   0.965                    0.99\n",
       "column_36_1  -0.0773  0.19    -0.4    0.69                    0.93\n",
       "column_37_1   0.0843  0.18    0.47    0.64                    1.09\n",
       "column_38_1  -0.2124   0.2   -1.08   0.281                    0.81\n",
       "column_39_1   0.2837  0.19     1.5   0.135                    1.33\n",
       "column_40_1   0.2625  0.17    1.52    0.13                     1.3\n",
       "column_41_1   0.0162  0.18    0.09    0.93                    1.02\n",
       "column_42_1   0.1502  0.19    0.77   0.439                    1.16\n",
       "column_43_1   0.3367  0.21     1.6   0.111                     1.4\n",
       "column_44_1   0.3104  0.18    1.69   0.093            *       1.36\n",
       "column_45_1   0.0132  0.19    0.07   0.944                    1.01\n",
       "column_46_1  -0.3019   0.2   -1.49   0.136                    0.74\n",
       "column_47_1   0.0215  0.19    0.11   0.912                    1.02\n",
       "column_48_1  -0.0671  0.22   -0.31   0.758                    0.94\n",
       "column_49_1   0.0232  0.19    0.12   0.903                    1.02\n",
       "column_50_1  -0.1686  0.19   -0.88    0.38                    0.84"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model3.get_inference(only_IC=False,ordered_columns_names=columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'LL': array([-113.97960799]),\n",
       " 'AIC_ll': array([329.95921597]),\n",
       " 'BIC_ll': array([544.90422899])}"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model3.get_inference(only_IC=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this table it can be seen that we modelled the probability of belonging to class 1\n",
    "\n",
    "ALL 5 first variables which are true variables ( conditionally to our DGP) are significant, but there are some false variables that are significant too, thus we would want to select them automatically\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also compare these results with official implementation of `statmodels`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.227959\n",
      "         Iterations 9\n",
      "                           Logit Regression Results                           \n",
      "==============================================================================\n",
      "Dep. Variable:                      0   No. Observations:                  500\n",
      "Model:                          Logit   Df Residuals:                      449\n",
      "Method:                           MLE   Df Model:                           50\n",
      "Date:                Sat, 06 Apr 2024   Pseudo R-squ.:                  0.6709\n",
      "Time:                        19:38:19   Log-Likelihood:                -113.98\n",
      "converged:                       True   LL-Null:                       -346.38\n",
      "Covariance Type:            nonrobust   LLR p-value:                 1.303e-68\n",
      "==============================================================================\n",
      "                 coef    std err          z      P>|z|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "const         -0.5041      0.197     -2.560      0.010      -0.890      -0.118\n",
      "0              2.8155      0.323      8.720      0.000       2.183       3.448\n",
      "1              1.4192      0.237      5.996      0.000       0.955       1.883\n",
      "2              0.5596      0.214      2.613      0.009       0.140       0.979\n",
      "3             -0.4007      0.191     -2.097      0.036      -0.775      -0.026\n",
      "4             -4.0397      0.453     -8.916      0.000      -4.928      -3.152\n",
      "5              0.3089      0.203      1.523      0.128      -0.089       0.706\n",
      "6             -0.7026      0.226     -3.107      0.002      -1.146      -0.259\n",
      "7             -0.1147      0.199     -0.576      0.565      -0.505       0.276\n",
      "8              0.0897      0.186      0.483      0.629      -0.274       0.453\n",
      "9             -0.2684      0.202     -1.327      0.184      -0.665       0.128\n",
      "10            -0.3067      0.202     -1.517      0.129      -0.703       0.090\n",
      "11             0.2999      0.181      1.656      0.098      -0.055       0.655\n",
      "12            -0.4047      0.194     -2.087      0.037      -0.785      -0.025\n",
      "13            -0.0524      0.198     -0.264      0.791      -0.441       0.336\n",
      "14            -0.4330      0.189     -2.297      0.022      -0.803      -0.063\n",
      "15            -0.2290      0.194     -1.181      0.238      -0.609       0.151\n",
      "16             0.0110      0.188      0.058      0.953      -0.358       0.380\n",
      "17             0.1072      0.189      0.568      0.570      -0.263       0.477\n",
      "18            -0.0747      0.169     -0.442      0.659      -0.406       0.257\n",
      "19            -0.0409      0.195     -0.210      0.834      -0.423       0.341\n",
      "20             0.1521      0.193      0.788      0.431      -0.226       0.530\n",
      "21            -0.2729      0.187     -1.457      0.145      -0.640       0.094\n",
      "22            -0.0843      0.209     -0.404      0.686      -0.493       0.325\n",
      "23            -0.3183      0.217     -1.464      0.143      -0.744       0.108\n",
      "24            -0.2431      0.201     -1.211      0.226      -0.637       0.150\n",
      "25             0.3631      0.215      1.689      0.091      -0.058       0.785\n",
      "26            -0.1852      0.205     -0.902      0.367      -0.588       0.217\n",
      "27             0.2104      0.202      1.043      0.297      -0.185       0.606\n",
      "28            -0.2582      0.195     -1.322      0.186      -0.641       0.125\n",
      "29             0.2441      0.200      1.222      0.222      -0.147       0.636\n",
      "30            -0.0049      0.184     -0.027      0.979      -0.366       0.356\n",
      "31             0.1585      0.198      0.802      0.423      -0.229       0.546\n",
      "32             0.1147      0.189      0.607      0.544      -0.255       0.485\n",
      "33            -0.0283      0.192     -0.147      0.883      -0.405       0.348\n",
      "34            -0.0086      0.199     -0.043      0.965      -0.398       0.381\n",
      "35            -0.0773      0.194     -0.399      0.690      -0.457       0.303\n",
      "36             0.0843      0.180      0.468      0.640      -0.269       0.437\n",
      "37            -0.2124      0.197     -1.079      0.281      -0.598       0.173\n",
      "38             0.2837      0.189      1.499      0.134      -0.087       0.655\n",
      "39             0.2625      0.173      1.518      0.129      -0.077       0.602\n",
      "40             0.0162      0.183      0.088      0.930      -0.342       0.375\n",
      "41             0.1502      0.194      0.774      0.439      -0.230       0.530\n",
      "42             0.3367      0.211      1.596      0.111      -0.077       0.750\n",
      "43             0.3104      0.184      1.685      0.092      -0.051       0.671\n",
      "44             0.0132      0.188      0.070      0.944      -0.356       0.382\n",
      "45            -0.3019      0.202     -1.493      0.135      -0.698       0.094\n",
      "46             0.0215      0.195      0.111      0.912      -0.360       0.403\n",
      "47            -0.0671      0.218     -0.308      0.758      -0.494       0.360\n",
      "48             0.0232      0.191      0.121      0.903      -0.351       0.398\n",
      "49            -0.1686      0.192     -0.878      0.380      -0.545       0.208\n",
      "==============================================================================\n"
     ]
    }
   ],
   "source": [
    "import statsmodels.api as sm\n",
    "\n",
    "logit_model = sm.Logit(pd.DataFrame(y), sm.add_constant(pd.DataFrame(x)))\n",
    "result = logit_model.fit()\n",
    "# Print summary of the model\n",
    "print(result.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Model Selection**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we will try to find the best model (set of variables) that minimise AIC or BIC.\n",
    "\n",
    "In order to have this set of variables, we will test different algorithms: \n",
    "- Backward regression\n",
    "- Forward regression \n",
    "- Stepwise regression\n",
    "\n",
    "This is done to minimise the risk of overfitting, reduce the model and find true variables that have a true impact on our target variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 2, 4, 6])"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index_cols=model3.autoselection(\"backward\",\"BIC_ll\",print_message=False)\n",
    "index_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([4, 0, 1, 2, 6])"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index_cols=model3.autoselection(\"forward\",\"BIC_ll\",print_message=False)\n",
    "index_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([6, 4, 0, 1, 2])"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index_cols=model3.autoselection(\"stepwise\",\"BIC_ll\",print_message=False)\n",
    "index_cols"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you see, all three methods give the same result that includes 4/5 true variables. Unfortunately the model selected one false variable and omitted one true variable that can be considered as mix of overfitting and underfitting"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
